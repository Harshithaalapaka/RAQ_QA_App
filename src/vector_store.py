import os
import uuid
import numpy as np
from typing import List, Any
import chromadb
class Vectorstore:
    def __init__(self,collection_name:str="pdf_docs",persist_directory:str="../data/vector_store"):
        self.collection_name=collection_name
        self.persist_directory=persist_directory
        self.client=None
        self.collection=None
        self._initialise_store()

    """By default, ChromaDB uses cosine distance.
    it finds vectors with the lowest cosine distance to your query vector.

However, note that:

Cosine distance = 1 - cosine similarity
distances ‚Äî lower = more similar.
Because most vector databases (including Chroma, FAISS, Milvus, etc.) internally treat "smaller = better" when searching nearest neighbors.

So instead of maximizing similarity, they minimize distance."""

    def _initialise_store(self):
        try:
            os.makedirs(self.persist_directory,exist_ok=True)
            self.client=chromadb.PersistentClient(path=self.persist_directory)
            self.collection=self.client.get_or_create_collection(
                name=self.collection_name,
                metadata={
                    "description":"pdf document embeddings for rag"
                }
            )
            print(f"vector store initialised.Collection:{self.collection_name}")
            print(f"existing documents in collection:{self.collection.count()}")
        except Exception as e:
            print(f"error initialising vector store:{e}")
            raise

    def add_documents(self,documents:List[Any],embeddings:np.ndarray):
        if len(documents)!=len(embeddings):
            raise ValueError("no of documents must match no of embeddings ")
        print(f"adding{len(documents)} documents to vector store...")

        ids=[]
        metadatas=[]
        documents_text=[]
        embeddings_list=[]

        for i,(doc,embedding) in enumerate(zip(documents,embeddings)):
            doc_id=f"doc_{uuid.uuid4().hex[:8]}_{i}"
            ids.append(doc_id)

            metadata=dict(doc.metadata)
            metadata['doc_index']=i
            metadata['content_length']=len(doc.page_content)
            metadatas.append(metadata)

            documents_text.append(doc.page_content)
            embeddings_list.append(embedding.tolist())

        try:
            self.collection.add(
                ids=ids,
                embeddings=embeddings_list,
                metadatas=metadatas,
                documents=documents_text
            )
            print(f"successfully added {len(documents)} documents to vectorstore")
            print(f"total docs in collection:{self.collection.count()}")

        except Exception as e:
            print(f"error adding docs to vectorstore:{e}")
            raise
"""vectorstore=Vectorstore()
vectorstore"""

"""What kind of ‚Äúdatabase mode‚Äù is used here?

You are using Local Vector Database ‚Äî specifically ChromaDB.

Let‚Äôs break it down clearly:

üîπ Type:

‚úÖ Local Vector Store (not API-based)

You are not sending embeddings or data to any online database.
Everything (embeddings, metadata, vectors) is stored locally on your disk inside the folder
self.client = chromadb.PersistentClient(path=self.persist_directory)
chromadb.PersistentClient ‚Üí means ChromaDB will store data on your local machine.

It automatically creates a folder structure that contains your vector embeddings and metadata files.

These files persist even if you close and reopen your program
Local (ChromaDB PersistentClient)	API-based (like Pinecone, Weaviate Cloud)
Storage	On your computer (e.g., ../data/vector_store/)	On remote cloud servers(to be paid based on usage)
.
so our current setup=local offline vector database+
online llm(groq api)

Local Embedding Model + Local Vector Store (ChromaDB) + Cloud LLM (Groq)
Your embedding model is local, not API-based ‚Äî unless you explicitly replaced it with something like OpenAIEmbeddings or HuggingFaceEmbeddings from LangChain.
You are using a local embedding model (like sentence-transformers/all-MiniLM-L6-v2)
‚úÖ Stored in local ChromaDB
‚úÖ Queried by local retriever
‚úÖ Answer generated by Groq API LLM

Your embedding model is local, not API-based ‚Äî unless you explicitly replaced it with something like OpenAIEmbeddings or HuggingFaceEmbeddings from LangChain.

if api based means:
from langchain.embeddings import OpenAIEmbeddings

class Embeddingmanager:
    def __init__(self):
        self.model = OpenAIEmbeddings(model="text-embedding-3-small")

    def generate_embeddings(self, texts):
        return self.model.embed_documents(texts)

you are using an API-based embedding model, hosted by OpenAI (or another provider).

üî∏ Meaning:

Requires an API key (e.g., OPENAI_API_KEY)

Needs internet.

Your data (text chunks) are sent to the provider‚Äôs server to generate embeddings.

So this mode =

üåê API Embedding Model + Local Vector Store (ChromaDB) + Cloud LLM (Groq)


"""





    
   
